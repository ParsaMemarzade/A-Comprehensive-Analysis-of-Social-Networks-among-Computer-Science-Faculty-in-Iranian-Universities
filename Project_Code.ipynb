{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links = pd.read_excel('links.xlsx')\n",
    "# urls = list(links['Link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x = []\n",
    "# fullName = []\n",
    "# fields = []\n",
    "# jobPosition = []\n",
    "# CitationsPreY = []\n",
    "# yearsOfCite = []\n",
    "# articlesinfo = []\n",
    "# CitationsBy = []\n",
    "# h_index = []\n",
    "# i10_index_2018 = []\n",
    "# h_index_2018 = []\n",
    "# i10_index = []\n",
    "# articles = []\n",
    "# yearsOfArticles = []\n",
    "# authors = []\n",
    "# Journals = []\n",
    "# nameArticles = []\n",
    "# Citations= []\n",
    "# Citations_2018= []\n",
    "# for i in urls:\n",
    " \n",
    "#     # driver = webdriver.Edge()\n",
    "#     # driver.get(i)\n",
    "#     # btn = driver.find_element_by_id(\"gsc_bpf_more\")\n",
    "#     # while btn.is_enabled()==True:\n",
    "#     #     btn.click()\n",
    "#     #     time.sleep(2)\n",
    "#     page = urlopen(i)\n",
    "#     html = page.read()\n",
    "#     soup = p(html)\n",
    "#     # content = driver.page_source\n",
    "#     # soup = p(content)\n",
    "    \n",
    "#     fullName.append(soup.find(id='gsc_prf_in').text)\n",
    "\n",
    "#     fieldsList = soup.find_all('a',class_='gsc_prf_inta gs_ibl')\n",
    "#     a = []\n",
    "#     for i in fieldsList:\n",
    "#         a.append(i.text)\n",
    "#     fields.append(','.join(a))\n",
    "\n",
    "#     jobPosition.append(soup.find('div',class_='gsc_prf_il').text)\n",
    "#     yearsOfCiteList = soup.find_all('span',class_='gsc_g_t')\n",
    "#     a = []\n",
    "#     for i in yearsOfCiteList:\n",
    "#             a.append(i.text)\n",
    "#     yearsOfCite.append(a)\n",
    "\n",
    "#     CitationsList = soup.find_all('a',class_='gsc_g_a')\n",
    "#     b = []\n",
    "#     for i in CitationsList:\n",
    "#             b.append(i.text)\n",
    "#     CitationsPreY.append(list(map(int, b)))\n",
    "\n",
    "#     t = soup.find_all('td',class_='gsc_rsb_std')\n",
    "#     Citations.append(int(t[0].text))\n",
    "#     h_index.append(int(t[2].text))\n",
    "#     i10_index.append(int(t[4].text))\n",
    "#     Citations_2018.append(int(t[1].text))\n",
    "#     h_index_2018.append(int(t[3].text))\n",
    "#     i10_index_2018.append(int(t[5].text))\n",
    "\n",
    "#     a = []\n",
    "#     c = []\n",
    "#     b = (soup.find_all('a',class_='gsc_a_at'))\n",
    "#     for i in b:\n",
    "#              if i.text == \"\":\n",
    "#                     a.append(\"null\")\n",
    "#              else:\n",
    "#                 a.append(i.text)\n",
    "#              c.append(soup.find(id='gsc_prf_in').text)\n",
    "#     articles.append(a)\n",
    "#     nameArticles.append(c)\n",
    "\n",
    "#     a = []\n",
    "#     b = (soup.find_all('td',class_='gsc_a_y'))\n",
    "#     for i in b:\n",
    "#              if i.text == \"\":\n",
    "#                     a.append(\"null\")\n",
    "#              else:\n",
    "#                 a.append(i.text)\n",
    "#     yearsOfArticles.append(a)\n",
    "\n",
    "#     a = []\n",
    "#     b = (soup.find_all('td',class_='gsc_a_c'))\n",
    "#     for i in b:\n",
    "#              if i.text == \"\":\n",
    "#                     a.append(\"null\")\n",
    "#              else:\n",
    "#                 a.append(i.text)\n",
    "#     CitationsBy.append(a)\n",
    "\n",
    "#     a = []\n",
    "#     b = soup.find_all('div',class_='gs_gray')\n",
    "#     for i in b:\n",
    "#              if i.text == \"\":\n",
    "#                     a.append(\"null\")\n",
    "#              elif i.text == \"0 articles\":\n",
    "#                   continue\n",
    "#              else:\n",
    "#                   a.append(i.text)\n",
    "#     b = []\n",
    "#     c = []\n",
    "#     for i in range(len(a)):\n",
    "#             if i%2==0:\n",
    "#               b.append(a[i])\n",
    "#             else:\n",
    "#               c.append(a[i])\n",
    "#     authors.append(b)\n",
    "#     Journals.append(c)   \n",
    "# infoData = {\n",
    "#     \"fullName\":fullName,\n",
    "#     \"fields\": fields,\n",
    "#     \"jobPosition\": jobPosition,\n",
    "#     \"Citations\" : Citations,\n",
    "#     \"h-index\" : h_index,\n",
    "#     \"i10-index\" : i10_index,\n",
    "#     \"Citations_2018\" : Citations_2018,\n",
    "#     \"h-index_2018\" : h_index_2018,\n",
    "#     \"i10-index_2018\" : i10_index_2018\n",
    "\n",
    "# }\n",
    "\n",
    "# infoDataDf = pd.DataFrame(infoData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nameArticles = [item for sublist in nameArticles for item in sublist]\n",
    "# articles = [item for sublist in articles for item in sublist]\n",
    "# authors = [item for sublist in authors for item in sublist]\n",
    "# yearsOfArticles = [item for sublist in yearsOfArticles for item in sublist]\n",
    "# Journals = [item for sublist in Journals for item in sublist]\n",
    "# infoArticles = {\n",
    "#         \"fullName\":nameArticles,\n",
    "#         \"Articles\": articles,\n",
    "#         \"authors\": authors,\n",
    "#         \"years\" : yearsOfArticles,\n",
    "#         \"Journals\" : Journals\n",
    "#     }\n",
    "# infoArticlesDf = pd.DataFrame(infoArticles)\n",
    "# infoArticlesDf.groupby(\"fullName\")\n",
    "# print(len(authors), len(Journals), len(articles), len(yearsOfArticles), len(nameArticles))\n",
    "# info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig , ax = plt.subplots()\n",
    "# ax.barh(yearsOfCite[1],CitationsPreY[3])\n",
    "# ax.set_title(infoDataDf.fullName[3])\n",
    "# ax.set_xlabel(\"Years\")\n",
    "# ax.set_ylabel(\"CitationsPerY\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup as p\n",
    "# from selenium import webdriver\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "# import matplotlib.pyplot as plt\n",
    "# from urllib.request import urlopen\n",
    "# from serpapi import GoogleSearch\n",
    "# import time\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stopWords1 = set(stopwords.words('english') + list(string.punctuation) + ['--','br','``','\\'\\''] + ['1','2','0','3','4','5','6','7','8','9']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "infoDataDf = pd.read_csv(\"infoData.csv\")\n",
    "infoDataDf = infoDataDf.drop(columns=['Unnamed: 0'], axis=1)\n",
    "infoDataDf = infoDataDf.dropna()\n",
    "infoDataDf['fields'] = infoDataDf['fields'].str.lower()\n",
    "infoDataDf['fields'] = infoDataDf['fields'].str.replace('internet of things','iot')\n",
    "infoDataDf['fields'] = infoDataDf['fields'].str.replace('artificial intelligence','ai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "infoDataDf['fields'] = infoDataDf['fields'].str.split(',')\n",
    "test = []\n",
    "liF = set()\n",
    "for i in infoDataDf['fields']:\n",
    "    for j in i:\n",
    "        liF.add(j)\n",
    "liV = list()\n",
    "for i in infoDataDf['fields']:\n",
    "    x = []\n",
    "    for j in liF:\n",
    "        if j in i:\n",
    "            x.append(1)\n",
    "        else:\n",
    "            x.append(0)\n",
    "    liV.append(x)\n",
    "\n",
    "VectorFileds = pd.DataFrame(liV,infoDataDf['fullName'],columns=liF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorFileds.to_csv('FiledsVector.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = list(VectorFileds.index)\n",
    "f = []\n",
    "for i in VectorFileds:\n",
    "    a = []\n",
    "    for j,x in zip(VectorFileds[i],VectorFileds[i].index):\n",
    "        if j==1:\n",
    "            a.append(x)\n",
    "    if len(a)>1:\n",
    "        f.append([i,a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "profs = set()\n",
    "vec = []\n",
    "for i in f:\n",
    "    for j in i[1]:\n",
    "        profs.add(j)\n",
    "for i in profs:\n",
    "    a = []\n",
    "    for j in profs:\n",
    "        c = 0\n",
    "        for z in f:\n",
    "            if i in z[1] and j in z[1] and i!=j:\n",
    "                c+=1\n",
    "        a.append(c)\n",
    "    vec.append(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProfSimilarity = pd.DataFrame(vec,profs,columns=profs)\n",
    "# test.to_csv('ProfSimilarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from matplotlib import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph = nx.DiGraph()\n",
    "\n",
    "for i in range(len(vec)):\n",
    "    Graph.add_node(list(profs)[i])\n",
    "    for j in range(i,len(vec)):\n",
    "        if vec[i][j]>1:\n",
    "            Graph.add_edge(list(profs)[i],list(profs)[j],weight=.75, value=20)\n",
    "        elif vec[i][j]==1:\n",
    "            Graph.add_edge(list(profs)[i],list(profs)[j],weight=.5, value=20)\n",
    "\n",
    "\n",
    "pos = nx.circular_layout(Graph)\n",
    "plt.figure(3,figsize=(200,200)) \n",
    "nx.draw(Graph,pos, with_labels = True)\n",
    "plt.savefig('Graph_circular.pdf',bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(Graph)\n",
    "plt.figure(3,figsize=(200,200)) \n",
    "nx.draw(Graph,pos, with_labels = True)\n",
    "plt.savefig('Graph_spring.pdf',bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc92dbe50645c2a0f0815161adfa607b251e3baf7875355af39ab3336211cbff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
